{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPm2fzqAomo2c37LjLZK55e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kilos11/Tensoflow-by-Ricardo-Solinzki/blob/main/Implementing_Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Analyzing Data\n",
        "# Statistical Regression"
      ],
      "metadata": {
        "id": "f8o7EotFyy8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pU0_PrC4Sa8",
        "outputId": "c166feec-9738-41d3-f18e-4d521115e806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.9.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-q1sIT3xVF8"
      },
      "outputs": [],
      "source": [
        "'''Analyzing Systems Using Regression:\n",
        "One of the most effective tools used by statisticians is regression. Regression ana￾lyzes a system by measuring the relationships between its variables. TensorFlow\n",
        "provides many capabilities for this analysis, and this chapter focuses on four types\n",
        "of regression:\n",
        "» Linear regression: Fitting a straight line to points in a dataset\n",
        "» Polynomial regression: Fitting a polynomial to points in a dataset\n",
        "» Binary logistic regression: Classifying points into one of two categories\n",
        "» Multinomial logistic regression: Classifying points into one of multiple\n",
        "categories\n",
        "The following sections explore these simple operations.'''\n",
        "\n",
        "'''Linear Regression: Fitting Lines to Data:\n",
        "An online search provides 40 selling prices that range from less than $5,000 to\n",
        "more than $10,000. Figure 6-1 illustrates these prices on a chart.\n",
        "Computing the average selling price would be easy, but you want to know whether\n",
        "the price is rising or falling and by how much the price is rising or falling. To find\n",
        "a good selling price, you decide to approximate your data with a line that indicates\n",
        "the change in the book’s price over time. This process is called linear regression,\n",
        "and the dashed gray line in Figure 6-1 identifies the general trend of the comic\n",
        "book’s price.\n",
        "The first step in TensorFlow training involves choosing an initial expression for\n",
        "the model (see Chapter 5). For linear regression, this decision is easy. The model\n",
        "is a line whose equation is y mx b, where m is the line’s slope, and b is the\n",
        "y-intercept (the y-value when x equals 0). The goal of linear regression is to deter￾mine m and b so that the resulting line best approximates (or fits) the set of points.\n",
        "The loss is also simple to compute. If the graph contains the point (x, y), the\n",
        "difference between the system and the model is y mx b .\n",
        "In machine learning applications, values of the loss should always have the same\n",
        "sign. You can make sure all the loss values are positive by computing the square of\n",
        "the error at each point and take the average of the error values.\n",
        "This method of computing loss is called the mean-squared error, or MSE. In Ten￾sorFlow, you can compute it by calling the reduce_mean function. The following\n",
        "code shows how this function is used:'''\n",
        "\n",
        "model = tf.add(tf.multiply(m, x), b)\n",
        "loss = tf.reduce_mean(tf.pow(model - y, 2))\n",
        "\n",
        "#To demonstrate this, the following code creates an optimizer, sets its learning rate\n",
        "#to 0.1, and calls its minimize method:\n",
        "optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
        "opt_op = optimizer.minimize(loss)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Linear Regression**"
      ],
      "metadata": {
        "id": "lV70Cxgr1Wa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Random input values\n",
        "N = 40\n",
        "x = tf.random_normal([N])\n",
        "m_real = tf.truncated_normal([N], mean=2.0)\n",
        "b_real = tf.truncated_normal([N], mean=3.0)\n",
        "y = m_real * x + b_real\n",
        "\n",
        "# Variables\n",
        "m = tf.Variable(tf.random_normal([]))\n",
        "b = tf.Variable(tf.random_normal([]))\n",
        "\n",
        "# Compute model and loss\n",
        "model = tf.add(tf.multiply(x, m), b)\n",
        "loss = tf.reduce_mean(tf.pow(model - y, 2))\n",
        "\n",
        "# Create optimizer\n",
        "learn_rate = 0.1\n",
        "num_epochs = 200\n",
        "num_batches = N\n",
        "optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(loss)\n",
        "\n",
        "# Initialize variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch session\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    # Perform training\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch in range(num_batches):\n",
        "            sess.run(optimizer)\n",
        "\n",
        "\n",
        "\n",
        "    # Display results\n",
        "    print('m = ', sess.run(m))\n",
        "    print('b = ', sess.run(b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "7KGq7OWl1ful",
        "outputId": "6a276317-1ce7-467b-fa9d-a42c024a25d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'tensorflow' has no attribute 'random_normal'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f5719e1dd53e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Random input values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mm_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mb_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'random_normal'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow.compat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "782U2G586lGy",
        "outputId": "0b1e1031-35fc-4572-dcc5-c719c9037541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow.compat (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow.compat\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "source": [
        "\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "# Random input values\n",
        "N = 40\n",
        "x = tf.random.normal([N])\n",
        "m_real = tf.truncated_normal([N], mean=2.0)\n",
        "b_real = tf.truncated_normal([N], mean=3.0)\n",
        "y = m_real * x + b_real\n",
        "\n",
        "# Variables\n",
        "m = tf.Variable(tf.random.normal([]))\n",
        "b = tf.Variable(tf.random.normal([]))\n",
        "\n",
        "# Compute model and loss\n",
        "model = tf.add(tf.multiply(x, m), b)\n",
        "loss = tf.reduce_mean(tf.pow(model - y, 2))\n",
        "\n",
        "# Create optimizer\n",
        "learn_rate = 0.1\n",
        "num_epochs = 200\n",
        "num_batches = N\n",
        "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learn_rate).minimize(loss)\n",
        "\n",
        "# Initialize variables\n",
        "init = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "# Launch session\n",
        "with tf.compat.v1.Session() as sess:\n",
        "    sess.run(init)\n",
        "    # Perform training\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch in range(num_batches):\n",
        "            sess.run(optimizer)\n",
        "\n",
        "    # Display results\n",
        "    print('m =', sess.run(m))\n",
        "    print('b =', sess.run(b))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "90QEozwB3evq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polynomial Regression: Fitting\n",
        "#Polynomials to Data"
      ],
      "metadata": {
        "id": "EQYHTQpi7p8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''You can easily extend the method of linear regression to polynomials. That is, the\n",
        "process of fitting a polynomial to a set of points uses essentially the same process\n",
        "as that used to fit a line.\n",
        "To demonstrate, I explain how you can approximate data with a cubic polynomial.\n",
        "You can express every cubic polynomial with the following equation:\n",
        "y = ax^3 + bx^2 + cx + d'''\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "#Polynomial Regression\n",
        "# Random input values\n",
        "N = 40\n",
        "x = tf.random_normal([N])\n",
        "a_real = tf.truncated_normal([N], mean=3.)\n",
        "b_real = tf.truncated_normal([N], mean=-2.)\n",
        "c_real = tf.truncated_normal([N], mean=-1.)\n",
        "d_real = tf.truncated_normal([N], mean=1.)\n",
        "y = a_real * tf.pow(x, 3) + b_real * tf.pow(x, 2) + c_real * x + d_real\n",
        "\n",
        "# Variables\n",
        "a = tf.Variable(tf.random_normal([]))\n",
        "b = tf.Variable(tf.random_normal([]))\n",
        "c = tf.Variable(tf.random_normal([]))\n",
        "d = tf.Variable(tf.random_normal([]))\n",
        "\n",
        "# Compute model and loss\n",
        "model = a * tf.pow(x, 3) + b * tf.pow(x, 2) + c * x + d\n",
        "loss = tf.reduce_mean(tf.pow(model - y, 2))\n",
        "\n",
        "# Create optimizer\n",
        "learn_rate = 0.01\n",
        "num_epochs = 400\n",
        "num_batches = N\n",
        "optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(loss)\n",
        "\n",
        "# Initialize variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch session\n",
        "with tf.Session() as sess:\n",
        " sess.run(init)\n",
        " # Perform training\n",
        " for epoch in range(num_epochs):\n",
        "    for batch in range(num_batches):\n",
        "        sess.run(optimizer)\n",
        "\n",
        "\n",
        "\n",
        " # Display results\n",
        " print('a = ', sess.run(a))\n",
        " print('b = ', sess.run(b))\n",
        " print('c = ', sess.run(c))\n",
        " print('d = ', sess.run(d))"
      ],
      "metadata": {
        "id": "Y48l9sPb79GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Binary Logistic Regression: Classifying Data into Two Categories\n"
      ],
      "metadata": {
        "id": "B6pqU9cQ-A6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''While linear and polynomial regression are concerned with identifying trends,\n",
        "logistic regression is concerned with placing data points into categories. If Points\n",
        "A and B belong to Category X and Points P and Q belong to Category Y, what cate￾gory will Point J belong to?\n",
        "The following sections look at systems with only two categories. Is the patient\n",
        "healthy or sick? Will the operation succeed or fail? This process of modeling sys￾tems with two categories is called binary logistic regression'''\n",
        "\n",
        "#Binary Logistic Regression\n",
        "# Input values\n",
        "N = 40\n",
        "x = tf.lin_space(0., 5., N)\n",
        "y = tf.constant([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "                1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
        "                1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
        "                1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
        "\n",
        "# Variables\n",
        "# Variables\n",
        "m = tf.Variable(0.)\n",
        "b = tf.Variable(0.)\n",
        "\n",
        "# Compute model and loss\n",
        "model = tf.nn.sigmoid(tf.add(tf.multiply(x, m), b))\n",
        "loss = -1. * tf.reduce_sum(y * tf.log(model) + (1. - y) * (1. - tf.log(model)))\n",
        "\n",
        "# Create optimizer\n",
        "learn_rate = 0.005\n",
        "num_epochs = 350\n",
        "optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(loss)\n",
        "\n",
        "# Initialize variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch session\n",
        "with tf.Session() as sess:\n",
        " sess.run(init)\n",
        " # Perform training\n",
        " for epoch in range(num_epochs):\n",
        "    sess.run(optimizer)\n",
        "\n",
        "\n",
        "\n",
        " # Display results\n",
        " print('m = ', sess.run(m))\n",
        " print('b = ', sess.run(b))\n"
      ],
      "metadata": {
        "id": "B6sGiiLZ-Zk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multinomial Logistic Regression:Classifying Data into Multiple Categories\n"
      ],
      "metadata": {
        "id": "dt-e4f6oAPfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Many machine learning applications need to classify points into more than two\n",
        "categories. This process is called multinomial logistic regression, and it resembles\n",
        "binary logistic regression in many respects. The primary difference is that it uses\n",
        "different functions to represent the model and loss.\n",
        "To present this topic, I explain how you can use TensorFlow to recognize hand￾writing samples from the Modified National Institute of Science and Technology\n",
        "(MNIST) dataset. Each image contains a handwritten digit that belongs to one of\n",
        "ten categories.\n",
        "\n",
        "The Modified National Institute of Science\n",
        "and Technology (MNIST) Dataset:\n",
        "If you decompress an MNIST file, you’ll see that each file stores its data in a single\n",
        "data structure. Thankfully, you don’t need to know anything about these struc￾tures because TensorFlow makes accessing MNIST data easy. The function to\n",
        "know is read_data_sets, which is provided by the tensorflow.contrib.learn.\n",
        "datasets.mnist package:'''\n",
        "\n",
        "read_data_sets(train_dir, fake_data=False, one_hot=False, dtype=dtypes.float32,\n",
        "reshape=True, validation_size=5000, seed=None)\n",
        "\n",
        "'''When this function executes, it searches for the four MNIST archives in the direc￾tory identified by the train_dir parameter. If any of the files can’t be found,\n",
        "read_data_sets will download them, decompress them, and store them in the\n",
        "specified folder To understand the other arguments of read_data_sets, it’s important to be\n",
        "familiar with the function’s return value, which is an instance of the Datasets\n",
        "class. Each Datasets instance has three fields:\n",
        "» train — a Dataset containing the MNIST training data\n",
        "» validation — a Dataset containing validation data\n",
        "» test — a Dataset containing data to be used for testing'''\n",
        "\n",
        "#The following code calls read_data_\n",
        "#sets, and for each field, it prints the shape of the corresponding image array:\n",
        "import tensorflow.contrib.learn as learn\n",
        "\n",
        "dset = learn.datasets.mnist.read_data_sets('MNIST-data')\n",
        "print(\"Training images: \", dset.train.images.shape)\n",
        "print(\"Validation images: \", dset.validation.images.shape)\n",
        "print(\"Test images: \", dset.test.images.shape)\n",
        "\n",
        "'''Defining the model with the\n",
        "softmax function:\n",
        "You can use the sigmoid function to classify points into two categories. (See the\n",
        "section “Defining models with the logistic function” for more information.) If a\n",
        "system (such as MNIST classification) has more than two categories, the sigmoid\n",
        "function won’t be sufficient.\n",
        "Instead, statisticians use an operation that can accept an array of values and\n",
        "return an array of values. This is the softmax function, which extends the sigmoid\n",
        "function to multiple variables.\n",
        "When using this function, you need to be aware of two points:\n",
        "» Each value in the output array lies between 0 and 1.\n",
        "» The sum of the values in the output array will always equal 1.\n",
        "In TensorFlow, you can perform the softmax operation by calling the softmax\n",
        "function in the tf.nn package:'''\n",
        "\n",
        "softmax(input, dim=-1, name=None)\n",
        "\n",
        "'''By default, every element of the input tensor is added together in the denominator\n",
        "of the softmax function. But if you set the dim parameter, only the values in the\n",
        "specified dimension will be included in the sum.'''\n",
        "\n",
        "'''Computing loss with cross entropy:\n",
        "In machine learning literature, this result is referred to as cross entropy. This term\n",
        "comes from information theory, and it refers to the usage of logarithms to deter￾mine how many bits should be used to represent messages. The following code\n",
        "defines a model by calling tf.nn.softmax and then computes the loss using cross\n",
        "entropy.'''\n",
        "\n",
        "model = tf.nn.softmax(tf.matmul(x, m) + b)\n",
        "loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(model))\n",
        "\n",
        "'''For improved performance, TensorFlow provides a function that combines the\n",
        "softmax function and cross entropy. This function is tf.nn.softmax_cross_\n",
        "entropy_with_logits and its signature is given as follows:'''\n",
        "\n",
        "softmax_cross_entropy_with_logits(labels=None, logits=None, dim=-1, name=None)\n",
        "\n",
        "'''You must identify each argument passed to this function by name. logits is set to\n",
        "the tensor that would be passed to softmax, and labels is set to a tensor contain￾ing the associated labels. logits and labels must have the same size.\n",
        "TensorFlow also provides a function that combines the sigmoid function and cross\n",
        "entropy: sigmoid_cross_entropy_with_logits. Its signature is given as follows:'''\n",
        "\n",
        "sigmoid_cross_entropy_with_logits(_sentinel=None, labels=None,\n",
        "logits=None, name=None)\n",
        "\n",
        "'''labels and logits accept the same values as the corresponding arguments of\n",
        "softmax_cross_entropy_with_logits.'''\n",
        "\n",
        "'''Putting theory into practice:\n",
        "The code in ch6/multi_regression.py demonstrates how you can use multino￾mial regression to load and classify images from the MNIST dataset. Listing 6-4\n",
        "presents the code.'''\n",
        "\n",
        "#Multinomial Logistic Regression\n",
        "# Read MNIST data\n",
        "dataset = learn.datasets.mnist.read_data_sets('MNIST-data', one_hot=True)\n",
        "# Placeholders for MNIST images\n",
        "image_holder = tf.placeholder(tf.float32, [None, 784])\n",
        "label_holder = tf.placeholder(tf.float32, [None, 10])\n",
        "# Variables\n",
        "m = tf.Variable(tf.zeros([784, 10]))\n",
        "b = tf.Variable(tf.zeros([10]))\n",
        "\n",
        "# Compute loss\n",
        "loss = tf.reduce_mean(\n",
        " tf.nn.softmax_cross_entropy_with_logits(\n",
        " logits=tf.matmul(image_holder, m) + b, labels=label_holder))\n",
        "\n",
        "# Create optimizer\n",
        "learning_rate = 0.01\n",
        "num_epochs = 25\n",
        "batch_size = 100\n",
        "num_batches = int(dataset.train.num_examples/batch_size)\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
        "# Initialize variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch session\n",
        "with tf.Session() as sess:\n",
        "     sess.run(init)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vtEHZVdMBFBg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}