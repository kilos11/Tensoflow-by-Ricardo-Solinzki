{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZ50dEVD+2PW2xoRPVzbQ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kilos11/Tensoflow-by-Ricardo-Solinzki/blob/main/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Formulating the Model**"
      ],
      "metadata": {
        "id": "R6go_fW_0jCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "AtiYUG6YwhqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Creating variables##\n",
        "# the following code creates a variable\n",
        "#named variableA and sets its initial value equal to a tensor named tensorA:\n",
        "tensorA = tf.constant([1.5, 2.5, 3.5])\n",
        "variableA = tf.Variable(tensorA)\n",
        "print(variableA)\n",
        "\n",
        "#The follow￾ing code creates a variable named variableB and sets its initial value to a tensor\n",
        "#of normally distributed values:\n",
        "'''variableB = tf.Variable(tf.random_normal([3]))'''\n",
        "\n",
        "\n",
        "##Initializing variables##\n",
        "'''variable_initializer(var_list,\n",
        "name=‘init’)'''#Returns an operation that initializes the variables in the given list\n",
        "'''local_variables_initializer()'''# Returns an operation that initializes all local variables\n",
        "'''global_variables_initializer()'''# Returns an operation that initializes all global variables\n",
        "\n",
        "\n",
        "##Determining Loss##\n",
        "\"\"\"This method of computing loss is called mean squared error, and it’s one of many\n",
        "methods available — maximum likelihood estimation and log likelihood estima￾tion are also popular. Chapter 6 discusses statistical regression and the different\n",
        "ways you can compute loss.\n",
        "If your model contains neural networks, you can’t compute loss with a simple\n",
        "equation. Feed-forward networks require a special algorithm like backpropagation,\n",
        "and recurrent networks rely on backpropagation through time (BPTT). I discuss\n",
        "neural networks and backpropagation in Chapter 7. I introduce BPTT in Chapter 9.\n",
        "There’s no right way to compute loss. The only requirement is that every decrease\n",
        "in loss must imply that the model is closer to the observed data. The process of\n",
        "improving the model by reducing loss is called optimization.\"\"\"\n",
        "\n",
        "\n",
        "##Minimizing Loss with Optimization##\n",
        "\"\"\"After you’ve formed an expression for the loss, the next step is to minimize the\n",
        "loss by updating the model’s variables. This process is called optimization, and\n",
        "TensorFlow supports a variety of algorithms for this purpose. Choosing the right\n",
        "algorithm is critically important when coding machine learning applications.\n",
        "Each optimization method is represented by a class in the tf.train package. Four\n",
        "popular optimization classes are the GradientDescentOptimizer, Momentum\n",
        "Optimizer, AdagradOptimizer, and AdamOptimizer classes. The following\n",
        "sections look at each of these classes, starting with the Optimizer class, which is\n",
        "the base class of TensorFlow’s optimization classes.\n",
        "The Optimizer class\n",
        "You can’t directly access the Optimizer class in code; applications need to instan￾tiate one of its subclasses instead. But the Optimizer class is crucial because it\n",
        "defines the all-important minimize method:\n",
        "minimize(loss, global_step=None, var_list=None, gate_gradients=1, aggregation_\n",
        "method=None, colocate_gradients_with_ops=False, name=None, grad_loss=None)\n",
        "The only required argument is the first, which identifies the loss. By default,\n",
        "minimize can access every trainable variable in the graph. An application can\n",
        "select specific variables for optimization by setting the var_list argument.\n",
        "minimize returns an operation that can be executed by a session’s run method.\n",
        "Each execution performs two steps:\n",
        "1. Compute values that update the variables of interest.\n",
        "2. Update the variables of interest with the values computed in Step 1.\n",
        "Just as you probably won’t win 20 Questions with your first question, you proba￾bly won’t optimize your model with a single call to minimize. Most applications\n",
        "perform optimization in a loop, and the following code gives an idea what an\n",
        "optimization loop looks like:\n",
        "\n",
        "# Create the optimizer and obtain the operation\n",
        "optimizer = tf.train.GradientDescentOptimizer(learn_rate)\n",
        "optimizer_op = minimize(loss)\n",
        "# Execute the minimization operation in a session\n",
        "with tf.Session() as sess:\n",
        " for step in range(num_steps):\n",
        " sess.run(optimizer_op)\n",
        "\n",
        "If the optimizer reaches a suitable minimum, it has converged to the minimum. If\n",
        "it fails to reach a minimum, the optimizer has diverged.\n",
        "Each call to the session’s run method minimizes the loss by updating variables.\n",
        "An application controls how updates are performed by creating a subclass of\n",
        "Optimizer. This discussion explores four popular Optimizer subclasses: Gradient\n",
        "DescentOptimizer, MomentumOptimizer, AdagradOptimizer, and AdamOptimizer.\n",
        "The following discussion gets awfully nerdy, and if you’re just getting started in\n",
        "machine learning, you don’t really need to know the math. However, selecting the\n",
        "right optimizer can make a significant impact on the application’s performance.\n",
        "Also, if you’re interviewing for a lucrative TensorFlow job, you should be able to\n",
        "justify why you prefer the AdamOptimizer to the GradientDescentOptimizer.\n",
        "\n",
        "The GradientDescentOptimizer:\n",
        "The GradientDescentOptimizer is the simplest and most common of the\n",
        "optimizers used in machine learning. If you look through online example code or\n",
        "textbooks on machine learning, you’re likely to encounter this optimizer\n",
        "frequently.\n",
        "Despite its popularity, few experts recommend the GradientDescentOptimizer\n",
        "over the alternatives. To see why, you need to understand the algorithm it uses to\n",
        "perform optimization. In this discussion, I present the theory of gradient descent\n",
        "and then explain how you can create and use GradientDescentOptimizers in\n",
        "code.\n",
        "\n",
        "Creating a GradientDescentOptimizer:\n",
        "An application can perform optimization with the gradient descent algorithm by\n",
        "creating a GradientDescentOptimizer. The constructor is given as follows:\n",
        "\n",
        "tf.train.GradientDescentOptimizer(learning_rate, use_\n",
        "locking=False, name='GradientDescent')\n",
        "\n",
        "The learning_rate parameter sets η, the learning rate. The following code cre￾ates an optimizer and sets its learning rate to 0.1:\n",
        "\n",
        "learn_rate = 0.1\n",
        "optimizer = tf.train.GradientDescentOptimizer(learn_rate)\n",
        "optimizer_op = optimizer.minimize(loss)\n",
        "\n",
        "Many developers set η using trial and error, and initial estimates frequently range\n",
        "between 0.1 and 0.0001. A common method is to start with a large value of η and\n",
        "reduce the value until the optimizer converges successfully. Computer scientists\n",
        "have devised automatic methods for selecting η, but to the best of my knowledge,\n",
        "no method has gained widespread acceptance.\n",
        "If you set the use_locking parameter to True, the GradientDescentOptimizer\n",
        "will acquire a lock that prevents other operations from modifying its variables.\n",
        "The variables can still be read normally.\n",
        "\n",
        "\n",
        "The MomentumOptimizer:\n",
        "The MomentumOptimizer has a lot in common with the GradientDescent\n",
        "Optimizer, but it usually converges faster with a reduced likelihood of oscillation.\n",
        "The MomentumOptimizer minimizes loss through the momentum algorithm, which\n",
        "uses preceding values of the loss gradient to update the current set of variables.\n",
        "The momentum algorithm introduces a new quantity that TensorFlow calls the\n",
        "accumulation. This quantity, denoted vt, is determined by the gradient of the cur￾rent loss, the learning rate, and the preceding value of the accumulation:\n",
        "vt t v J\n",
        "An application can create a MomentumOptimizer by calling its constructor:\n",
        "\n",
        "MomentumOptimizer(learning_rate, momentum, use_locking=False, name='Momentum',\n",
        "use_nesterov=False)\n",
        "\n",
        "The use_locking parameter has the same purpose as the use_locking parameter\n",
        "in the GradientDescentOptimizer constructor. That is, the optimizer will lock its\n",
        "variables’ values if use_locking is set to True.\n",
        "If use_nesterov is set to True, the optimizer adopts the Nesterov Accelerated\n",
        "Gradient descent algorithm, which is commonly shortened to NAG. The NAG algo￾rithm modifies the momentum algorithm by updating variables before computing\n",
        "the loss.\n",
        "\n",
        "\n",
        "The AdagradOptimizer:\n",
        "The gradient descent algorithm and the momentum algorithm apply the same\n",
        "learning rate to each variable being trained. But different variables may converge\n",
        "to their minima at different rates. The adaptive gradient (Adagrad) algorithm\n",
        "takes this into account.\n",
        "The Adagrad algorithm has two characteristics that have made it popular among\n",
        "academics and experts:\n",
        "» The learning rate changes from variable to variable and from step to step. The\n",
        "learning rate at the tth step for the ith variable is denoted t i, .\n",
        "» Adagrad methods compute subgradients instead of gradients. A subgradient is\n",
        "a generalization of a gradient that applies to nondifferentiable functions. This\n",
        "means AdaGrad methods can optimize both differentiable and nondifferen￾tiable functions.\n",
        "Thankfully, TensorFlow developers don’t have to worry about subgradients or\n",
        "outer products. This is because the TensorFlow API provides the Adagrad\n",
        "Optimizer class, whose constructor is given as follows.\n",
        "\n",
        "AdagradOptimizer(learning_rate, initial_accumulator_value=0.1, use_locking=False,\n",
        "name='Adagrad')\n",
        "\n",
        "One shortcoming of the Adagrad algorithm is that the learning rates always\n",
        "decrease in magnitude. As training continues, their values will eventually reach\n",
        "zero, bringing training to a halt.\n",
        "\n",
        "\n",
        "The AdamOptimizer:\n",
        "The Adam (Adaptive Moment Estimation) algorithm closely resembles the Adag￾rad algorithm in many respects.\n",
        "It also resembles the Momentum algorithm\n",
        "because it takes two factors into account:\n",
        "» The first moment vector: Scales the gradient by 1 -1\n",
        "» The second moment vector: Scales the square of the gradient by 1 -2\n",
        "To employ the Adam algorithm, you need to create an instance of AdamOptimizer.\n",
        "The constructor is given as follows:\n",
        "\n",
        "AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08,\n",
        "use_locking=False, name='Adam')\n",
        "\n",
        "When deciding on an optimizer, I always start with the AdamOptimizer, especially\n",
        "when working with images. \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtviDQ3Z1MsK",
        "outputId": "3d8801f6-50b1-49ce-fc09-124d2eee5c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'Variable:0' shape=(3,) dtype=float32>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feeding Data into a Session**"
      ],
      "metadata": {
        "id": "WVCp9e-v-QTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Instead of processing all the test data with one call to a session’s run method,\n",
        "applications frequently split the data into portions and call run once for each por￾tion. There are at least three reasons to do so:\n",
        "» If the data is stored in a file or on a remote server, it may be more efficient to\n",
        "process one portion of data while another is loaded from the source.\n",
        "» Shuffling the portions of data increases the data’s stochasticity. This process\n",
        "can improve convergence to a global minimum instead of a local minimum.\n",
        "I explain the rationale for data shuffling in the upcoming “Stochasticity” section.\n",
        "» Time constraints make it impractical to process all the data at once.\n",
        "A portion of data processed in one session execution is called a batch. The process\n",
        "of transferring batches to a session is called feeding data to the session. To configure\n",
        "this in code, an application needs to perform three steps:\n",
        "1. Define placeholders to contain the data to be fed into the session.\n",
        "2. Use the placeholders in the expressions for model and loss.\n",
        "3. Set the second parameter of the session’s run method to a dictionary\n",
        "that associates each placeholder with a source of data.\n",
        "Step 2 is trivial because you can process placeholders in the same way that you can\n",
        "process tensors. This discussion focuses on Steps 1 and 3. Later chapters present\n",
        "code that demonstrates how data can be fed into a session.\n",
        "\n",
        "Creating placeholders:\n",
        "A placeholder is a constant Tensor that holds a batch of data to be fed into a\n",
        "session. You can create placeholders by calling the tf.placeholder function:\n",
        "\n",
        "tf.placeholder(dtype, shape=None, name=None)\n",
        "\n",
        "The first two arguments specify the type of the placeholder’s elements and its\n",
        "size. The actual content of a placeholder is set by the running session, so there’s\n",
        "no way to initialize a placeholder.\n",
        "For example, the following code creates a placeholder that contains 32-bit\n",
        "floating-point values:\n",
        "\n",
        "ph = tf.placeholder(tf.float32)\n",
        "\n",
        "If a placeholder’s shape isn’t given, it can be set to a tensor of any shape. If the\n",
        "shape is given, assigning a tensor of a different shape will cause an error.\n",
        "When associating data with a placeholder, there’s a catch: The data source can be\n",
        "a list of constants or a NumPy ndarray, but it can’t be a tensor. The following code\n",
        "associates a placeholder with an ndarray:'''\n",
        "\n",
        "ph = tf.placeholder(tf.float32)\n",
        "vals = np.array([9., 8., 7.])\n",
        "incr = tf.add(ph, 1.)\n",
        "with tf.Session() as sess:\n",
        " res = sess.run(incr, feed_dict={ph: vals})\n",
        " print(res)\n",
        "\n",
        " '''Find name of all students in the table,\n",
        " Fetch current date-time from the system, create a table called heroes'''\n",
        "\n",
        "\n",
        " '''Stochasticity:\n",
        " To keep optimizers from converging to a local minimum instead of a global mini￾mum, many applications split their training data into small batches and feed them\n",
        "randomly to the session. This randomness, also called stochasticity, forces the opti￾mizer to take larger jumps at first and smaller jumps as training progresses. This\n",
        "jumping increases the likelihood that the optimizer will find a global minimum.\n",
        "If the gradient descent algorithm is employed to process stochastic data, it’s\n",
        "referred to as the stochastic gradient descent algorithm. If you encounter the term\n",
        "SGD in machine learning literature, this algorithm is what it’s referring to.'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G8I7N3L_-Y1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monitoring Steps, Global Steps,\n",
        "and Epochs:\n"
      ],
      "metadata": {
        "id": "Ytsy39D9x0lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Monitoring Steps, Global Steps,\n",
        "and Epochs:\n",
        "In TensorFlow, each session execution that processes a single batch of data is\n",
        "called a step. Many TensorFlow functions and methods accept a parameter called\n",
        "global_step, which can be used to monitor the total number of steps executed by\n",
        "a session. In practice, global_step serves as the index of the batch being pro￾cessed. You can access this index in code by calling tf.train.global_step.\n",
        "You can also store the global step in a regular variable. This storage requires two\n",
        "operations:\n",
        "1. Define a variable with an initial value of 0 and its trainable argument\n",
        "set to False.\n",
        "2. Set the variable equal to the global_step parameter of the optimizer’s\n",
        "minimize method.\n",
        "If its global_step parameter is set to a variable, minimize will increment the\n",
        "variable each time a session processes a batch of data. The following code creates\n",
        "a variable named gstep and configures it to store the application’s global step:'''\n",
        "\n",
        "# Define the variable to hold the global step\n",
        "gstep = tf.Variable(0, trainable=False)\n",
        "# Configure the optimizer\n",
        "learn_rate = 0.1\n",
        "batch_size = 40\n",
        "optimizer = tf.train.GradientDescentOptimizer(learn_rate).\n",
        " minimize(loss, global_step=gstep)\n",
        "# Initialize variables\n",
        "init = tf.global_variables_initializer()\n",
        "# Launch session\n",
        "with tf.Session() as sess:\n",
        " sess.run(init)\n",
        " for batch in range(batch_size):\n",
        " _, step, result = sess.run([optimizer, gstep, x_min])\n",
        " print(\"Step %d: Computed result = %f\" % (step, result))\n",
        "\n",
        " '''As you look at this code, a question may occur to you: Why keep track of the global\n",
        "step when you can access the loop index? To answer this question, suppose that\n",
        "you execute ten training batches and then restart your application. The loop vari￾able will revert back to 0, but if you’d saved the global step to a file, you can\n",
        "restore it and use it as the current global step. I explain how to save variables to a\n",
        "file in the section “Saving variables,” later in this chapter.\n",
        "In the preceding example, the test executes each batch only once. In a real-world\n",
        "application, all the batches will be processed multiple times. A pass through every\n",
        "batch of a dataset is referred to as an epoch. For example, if a dataset is split into\n",
        "50 batches, an epoch consists of 50 steps.\n",
        "Many applications execute sessions in two loops: The outer loop iterates once for\n",
        "each epoch, and the inner loop executes once for each batch. The following code\n",
        "creates the two loops and calls sess.run with each iteration:'''\n",
        "\n",
        "for epoch in range(epochs):\n",
        " for batch in range(batch_size):\n",
        " _, step, result = sess.run([optimizer, gstep, x_min])\n",
        " print(\"Step %d: Computed result = %f\" % (step, result))"
      ],
      "metadata": {
        "id": "XkP2aCudyF-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Saving and Restoring Variables**"
      ],
      "metadata": {
        "id": "1QcYzBQWygfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''The Saver class makes it straightforward to load and store variables. By default, a\n",
        "Saver can access every variable in the session. But the first argument of the con￾structor can identify specific variables to be accessed. For example, the following\n",
        "code creates a Saver that can save/restore only two variables: firstVar and\n",
        "secondVar:'''\n",
        "\n",
        "saver = tf.train.Saver([firstVar, secondVar])\n",
        "\n",
        "'''After you create a Saver, you can store variables to a file by calling its save method.\n",
        "Then you can restore variables from a file by calling restore.'''\n",
        "\n",
        "'''Saving variables:\n",
        "The save method stores variables and data related to the variables. By default, the\n",
        "method creates at least three binary files, each with the same name but a different\n",
        "suffix:\n",
        "» filename.data-X-of-Y: Stores variable values\n",
        "» filename.index: Holds the offset of each variable in the data file(s)\n",
        "» filename.meta: MetaGraphDef containing the structure of the graph that\n",
        "contains the variables The data files contain variable values, and if the application has many variables,\n",
        "save will create multiple data files. If there’s only one file, its name will be\n",
        "filename.data-00000-of-00001.\n",
        "The index file contains a table that matches variable names to offsets in the index\n",
        "file. You can retrieve variables using the restore method, which I explain in the\n",
        "next section.\n",
        "You can create these files by creating a Saver and calling its save method:'''\n",
        "\n",
        "save(sess, save_path, global_step=None, latest_filename=None,\n",
        "meta_graph_suffix='meta', write_meta_graph=True, write_\n",
        "state=True)\n",
        "\n",
        "'''These parameters are straightforward to understand. sess is the session contain￾ing the variables of interest and save_path identifies the path of the file to contain\n",
        "the saved data. The last element of save_path specifies the name of the files to be\n",
        "generated.\n",
        "If latest_filename is set, save will create a text file that lists the paths of files\n",
        "involved in the save operation. If global_step is set, the value will be appended\n",
        "to each of the generated files.\n",
        "For example, the following code creates a Saver and calls save to create the\n",
        "generated files (output.*) in the current directory:'''\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "saver.save(sess, os.getcwd() + \"/output\")\n",
        "\n",
        "'''If there aren’t many variables to store, save will generate only three files:\n",
        "output.data-00000-of-00001, output.index, and output.meta.'''\n",
        "\n",
        "'''Restoring variables:\n",
        "The restore method loads variables that have been stored previously. The process\n",
        "of restoring variables consists of two steps:\n",
        "1. Call import_meta_graph to add the variables’ nodes to the current graph.\n",
        "2. Call restore to access the variable data.\n",
        "The first step is simple. tf.train.import_meta_graph accepts the path of a\n",
        "*.meta file, reads graph data from the file, and adds the graph’s nodes to the cur￾rent graph. The function returns a Saver that lets you restore variables from the\n",
        "loaded graph.\n",
        "For example, the following code imports graph data from output.meta and\n",
        "obtains a Saver that can be used to load variables:'''\n",
        "\n",
        "saver = tf.train.import_meta_graph(\"output.meta\")\n",
        "\n",
        "'''After obtaining the Saver, an application can load variables by calling its restore\n",
        "method, whose signature is given as follows:'''\n",
        "\n",
        "restore(sess, save_path)\n",
        "\n",
        "'''As in the save method, sess identifies the session containing the variables, and\n",
        "save_path is the path to the file containing the variable data. This path must\n",
        "include the name of the three files without the suffix. As an example, the follow￾ing\n",
        "code uses saver to load variables from output into the current graph:'''\n",
        "\n",
        "saver.restore(sess, os.getcwd() + \"/output\")\n",
        "\n"
      ],
      "metadata": {
        "id": "RJho5xZzyk34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Working with SavedModels**"
      ],
      "metadata": {
        "id": "QqmjN--5aZ_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''In addition to storing variables with a Saver, you can store your application’s entire\n",
        "model by creating a SavedModel. As stated in the documentation, SavedModels are\n",
        "“the universal serialization format for TensorFlow models” and serve as “the\n",
        "canonical way to export TensorFlow graphs.”\n",
        "To be precise, a SavedModel is a directory that contains a *.pb or *.pbtxt file. This\n",
        "file contains the application’s model and stores graph definitions in MetaGraphDef\n",
        "protocol buffers. In addition to this file, a SavedModel may contain one or more of\n",
        "the following subdirectories:\n",
        "» variables: A directory containing the application’s variables (files are similar to\n",
        "those produced by the Saver’s save method, excluding the *.meta file)\n",
        "» assets: Auxiliary files that need to be loaded into the graph\n",
        "» assets.extra: User-provided files that don’t need to be loaded into the graph\n",
        "Saving and restoring a SavedModel isn’t conceptually difficult, but the code gets a\n",
        "little complicated.'''\n",
        "\n",
        "'''Saving a SavedModel:\n",
        "The process of saving an application’s model to a SavedModel is similar to the\n",
        "process of storing variables. But instead of creating a Saver, you need to create a\n",
        "tf.saved_model.builder.SavedModelBuilder. The constructor accepts a single\n",
        "argument that identifies the top-level directory:'''\n",
        "\n",
        "builder = tf.saved_model.builder.SavedModelBuilder(\"out\")\n",
        "\n",
        "'''After creating a SavedModelBuilder, you can add data to the model and save the\n",
        "model to the given directory. To add data to the model, you need to call one of two\n",
        "functions: add_meta_graph or add_meta_graph_and_variables. The signature of\n",
        "add_meta_graph is given as'''\n",
        "\n",
        "add_meta_graph(tags, signature_def_map=None, assets_\n",
        "collection=None, legacy_init_op=None, clear_devices=False,\n",
        "main_op=None)\n",
        "\n",
        "'''Metagraphs identify their capabilities and purposes with strings called tags. You\n",
        "can assign a metagraph’s tags by setting the tags parameter. The tf.saved_\n",
        "model.tag_constants provides three common tags: GPU, SERVING, and TRAINING.\n",
        "A graph’s inputs and outputs form its signature. In code, a graph’s signature is\n",
        "represented by a SignatureDef, and you can create this by calling the build_\n",
        "signature_def function of the tf.saved_model.signature_def_utils package:'''\n",
        "\n",
        "build_signature_def(inputs=None, outputs=None, method_name=None)\n",
        "\n",
        "'''To create the signature, you need to set inputs and outputs to dictionaries that\n",
        "associates names with TensorInfo protocol buffers. For the names, many appli￾cations use constants from tf.saved_model.signature_constants, which\n",
        "include CLASSIFY_INPUTS, CLASSIFY_OUTPUT_CLASSES, PREDICT_INPUTS, and\n",
        "PREDICT_OUTPUTS.\n",
        "You can obtain a TensorInfo for a tensor by calling tf.saved_model.utils.\n",
        "build_tensor_info with the tensor. The following code returns a TensorInfo for\n",
        "a tensor named input_vec:'''\n",
        "\n",
        "info = tf.saved_model.utils.build_tensor_info(input_vec)\n",
        "\n",
        "'''The method_name parameter of build_signature_def is a string that serves as\n",
        "the signature’s method name. You can set this to one of the strings in the\n",
        "tf.saved_model.signature_constants module, such as CLASSIFY_METHOD_\n",
        "NAME, PREDICT_METHOD_NAME, or REGRESS_METHOD_NAME.\n",
        "The add_meta_graph_and_variables method is similar to add_meta_graph, but it\n",
        "has an extra parameter. The first parameter of add_meta_graph_and_variables is\n",
        "sess, which identifies the session that should provide the metagraph’s variables.\n",
        "After you’ve added metagraphs to a SavedModel, you can store the SavedModel by\n",
        "calling the save method. This accepts an as_text parameter that identifies\n",
        "whether the protocol buffer should be saved as a text file (*.pbtxt) or a binary file\n",
        "(*.pb). By default, save stores metagraph data in a binary file.'''\n",
        "\n",
        "'''Loading a SavedModel:\n",
        "While it’s complex to save metagraphs to a SavedModel, it’s easy to load them.\n",
        "You need to know only one function:'''\n",
        "\n",
        "tf.saved_model.loader.load(sess, tags, export_dir, **saver_kwargs)\n",
        "\n",
        "'''This loads the MetaGraphDef protocol buffer from the directory given by export_\n",
        "dir with the tags given by tags. The sess parameter identifies the session that\n",
        "should contain the metagraph’s variables, assets, and signatures.'''\n"
      ],
      "metadata": {
        "id": "nBEd0m-XadnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Putting Theory into Practice**"
      ],
      "metadata": {
        "id": "TenXBmYub9BD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training in TensorFlow\n",
        "# Define a trainable variable\n",
        "x_var = tf.Variable(0., name='x_result')\n",
        "\n",
        "# Define an untrainable variable to hold the global step\n",
        "step_var = tf.Variable(0, trainable=False)\n",
        "\n",
        "# Express loss in terms of the variable\n",
        "loss = x_var * x_var - 4.0 * x_var + 5.0\n",
        "\n",
        "# Find variable value that minimizes loss\n",
        "learn_rate = 0.1\n",
        "num_epochs = 40\n",
        "optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(loss, global_\n",
        "step=step_var)\n",
        "\n",
        "# Initialize variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Create the saver\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "# Create summary data and FileWriter\n",
        "summary_op = tf.summary.scalar('x', x_var)\n",
        "file_writer = tf.summary.FileWriter('log', graph=tf.get_default_graph())\n",
        "\n",
        "# Launch session\n",
        "with tf.Session() as sess:\n",
        " sess.run(init)\n",
        " for epoch in range(num_epochs):\n",
        "    _, step, result, summary = sess.run([optimizer, step_var, x_var,\n",
        "summary_op])\n",
        "    print('Step %d: Computed result = %f' % (step, result))\n",
        "\n",
        "    # Print summary data\n",
        "    file_writer.add_summary(summary, global_step=step)\n",
        "    file_writer.flush()\n",
        "# Store variable data\n",
        " saver.save(sess, os.getcwd() + '/output')\n",
        " print('Final x_var: %f' % sess.run(x_var))\n",
        "\n",
        " '''This code creates two variables: a trainable variable named x_var and an untrain￾able variable named step_var. loss is set to a quadratic equation whose indepen￾dent variable is x_var.\n",
        "The application calls tf.global_variables_initializer to obtain an operation\n",
        "for initializing its variables. The session must execute this operation before it can\n",
        "train the variables in the optimization process.\n",
        "After creating the variables, the application creates a GradientDescentOptimizer\n",
        "and calls its minimize method to reduce the loss to a minimum. Then it assigns\n",
        "the global_step parameter of minimize to step_var. This assignment tells the\n",
        "session to increment step_var each time it performs a training operation.\n",
        "After each training operation, print displays the global step and the current value\n",
        "of x_var. As training continues, x_var approaches 2, the point of minimum\n",
        "loss.  Similarly, step_var approaches 40 because the training loop performs\n",
        "40 iterations.\n",
        "When optimization is complete, the application stores its variables to a file. The\n",
        "save method of the Saver instance stores variable data to three files in the current\n",
        "directory: output.data-00000-of-00001, output.index, and output.meta.\n",
        "The code in ch5/restore_vars.py loads the value of x_var from the new data\n",
        "files. Listing 5-2 presents the code.'''\n",
        "\n",
        "#Loading Variables from a File\n",
        "# Create session\n",
        "with tf.Session() as sess:\n",
        " # Load stored graph into current graph\n",
        " saver = tf.train.import_meta_graph('output.meta')\n",
        " # Restore variables into graph\n",
        " saver.restore(sess, os.getcwd() + '/output')\n",
        " # Display value of variable\n",
        " print('Variable value: ', sess.run('x_result:0'))\n",
        "\n",
        " '''It’s important to see that this code doesn’t create a Saver by calling the class’s\n",
        "constructor. Instead, it obtains a Saver by calling import_meta_graph with the\n",
        "name of the file containing graph data.\n",
        "After obtaining the Saver, the application obtains the variable’s value by calling\n",
        "the Saver’s restore method and the session’s run method. Even though the vari￾able’s name was x_var, the application calls run with x_output:0 because the\n",
        "variable’s name parameter was set to x_output.'''\n"
      ],
      "metadata": {
        "id": "_W3W4U3ycAvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualizing the Training Process**"
      ],
      "metadata": {
        "id": "EpvkVG9kvhMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The ch5/simple_train.py application prints the loss at each step using the fol￾lowing code:\n",
        "print(\"Step %d: Computed result = %f\" % (step, result))\n",
        "\n",
        "'''TensorFlow provides a better way to monitor training. Chapter  4 covers the\n",
        "TensorBoard utility, which reads summary data generated by an application. The\n",
        "code in ch5/simple_train.py generates summary data for training by perform￾ing four steps:\n",
        "1. Call tf.summary.scalar to create an operation that writes x_var to\n",
        "summary data.\n",
        "2. Call tf.summary.FileWriter to create a FileWriter.\n",
        "3. Execute the session with the operation from Step 1.\n",
        "4. With each session execution, print the summary data by calling the\n",
        "FileWriter’s add_summary method'''\n",
        "#For the last step, the following code prints the summary data:\n",
        "file_writer.add_summary(summary, global_step=step)\n"
      ],
      "metadata": {
        "id": "BteRNFH0vmIw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}