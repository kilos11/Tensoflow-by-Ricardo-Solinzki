{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPO+4ptOLioTLMaYJg+JVQV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kilos11/Tensoflow-by-Ricardo-Solinzki/blob/main/Introducing_Neural_Networks_and_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4F-lMCD0Z1G"
      },
      "outputs": [],
      "source": [
        "##From Neurons to Perceptrons##\n",
        "'''\n",
        "Neurons:\n",
        "» A neuron receives one or more incoming signals and produces one outgo￾ing signal.\n",
        "» A neuron’s output can serve as the input of another neuron.\n",
        "» Every neuron has a threshold, and the neuron won’t produce output until its\n",
        "electricity exceeds the threshold.\n",
        "\n",
        "Perceptron:\n",
        "» A perceptron is a linear neuron with a single input and a single output.\n",
        "» Like a neuron, a perceptron receives multiple inputs and produces one output. But\n",
        "a perceptron’s inputs are provided as numeric values instead of electrical pulses.\n",
        "Similarly, the perceptron’s threshold value is represented by a number. If the sum\n",
        "of the inputs exceeds the threshold, the perceptron’s output will be 1. If the sum\n",
        "of the signals falls below the threshold, the output will be 0.\n",
        "For example, suppose that x0 is set to 0.5, x1\n",
        " is set to 1.5, x2 is set to 2.5, and x3 is\n",
        "set to -1.0. The sum of the signals is 3.5. If the perceptron’s threshold value is 3.0,\n",
        "the perceptron will produce an output of 1. If the threshold value is 4.0, the per￾ceptron will produce an output of 0.'''\n",
        "\n",
        "##Improving the Model##\n",
        "'''\n",
        "» Each incoming signal is assigned a weight that indicates its influence.\n",
        "» Instead of a threshold value, a constant called a bias is added to the incoming\n",
        "signals.\n",
        "» The sum of weighted inputs is passed to an activation function that determines\n",
        "the output.\n",
        "These changes make neural networks suitable for machine learning. Modern\n",
        "developers refer to the elements of these networks as nodes instead of perceptrons.\n",
        "\n",
        "Weights:\n",
        "To determine the total effect of the inputs, a node multiplies each input by its\n",
        "weight and adds the products together. Then it compares the sum to its threshold.\n",
        "If the sum is greater than the threshold, the node produces an output value equal\n",
        "to 1. If not, the output value is 0.\n",
        "Mathematically, weights are denoted as wi\n",
        ", where i represents the weight of the\n",
        "ith input.\n",
        "Weights play a vital role in machine learning because they enable an application\n",
        "to update the neural network’s behavior. As an application performs training, it\n",
        "updates the weights to improve the model.\n",
        "\n",
        "Bias:\n",
        "In machine learning, bias is a constant that is added to the weighted sum of the\n",
        "inputs. Bias is used to make the model more resilient to noise and outliers.\n",
        "The bias receives a weight just like every other input. For this reason, it makes sense\n",
        "to set the bias’s value to 1, which is why the lowest node on the left is given as +1.\n",
        "\n",
        "Activation functions:\n",
        "The unit step function is simple to understand, but it’s not practical for machine\n",
        "learning. Computer scientists have devised many more suitable functions for pro￾ducing a\n",
        "perceptron’s output, and they’re called activation functions.\n",
        "A node’s activation function accepts the weighted sum of the node’s inputs and\n",
        "produces a single output value. In TensorFlow, an activation function accepts a\n",
        "tensor of values and returns a tensor containing output values.\n",
        "I like to divide these functions into two categories: rectifiers and classifiers. The\n",
        "distinction is simple: If a node’s output identifies a category, set its activation\n",
        "function to a classifier. Otherwise, set the node’s activation function to a rectifier.\n",
        "\n",
        "Activation Functions:\n",
        "Activation Function              Description\n",
        "tf.nn.relu(input, name=None)     Returns the input value if positive, returns 0 otherwise\n",
        "tf.nn.relu6(input, name=None)    Returns the input value if positive, up to a maximum of 6. Returns\n",
        "0 otherwise\n",
        "tf.nn.crelu(input, name=None)    Returns a concatenated tensor that separates the positive and\n",
        "negative portions of the input\n",
        "tf.nn.elu(input, name=None)      Returns the input value if positive, returns the exponential of the\n",
        "input otherwise\n",
        "tf.nn.sigmoid(input, name=None)  Returns 1/(1 + exp(-x))\n",
        "tf.nn.tanh(input, name=None)     Returns tanh(x)\n",
        "tf.nn.softsign(input, name=None) Returns x/(abs(x) + 1)\n",
        "\n",
        "Rectifier functions:\n",
        "In an electrical circuit, a rectifier accepts an input signal and transmits an equal\n",
        "output signal if the input is positive. If the input signal is negative, the rectifier\n",
        "transmits an output of zero.\n",
        "The rectified linear unit function, or ReLU, performs a similar operation. It returns\n",
        "the input if it’s positive and returns 0 otherwise. Put another way, the ReLU\n",
        "function returns the maximum of the input and 0.\n",
        "In TensorFlow, applications can perform ReLU operations by calling tf.nn.relu.\n",
        "The ELU in tf.nn.elu stands for Exponential Linear Unit. This activation function\n",
        "returns the input value if it’s greater than zero. If the input is zero or less,\n",
        "tf.nn.elu returns the exponential of the input minus one.\n",
        "'''\n",
        "\n",
        "##Layers and Deep Learning##\n",
        "'''\n",
        "Layers:\n",
        "The columns of a neural network are referred to as layers, and for this reason,\n",
        "neural networks are frequently called multilayer perceptrons (MLPs). Every\n",
        "neural net has at least two layers.\n",
        "The layers of a neural network have specific names.\n",
        "The first layer, which pro￾vides input values, is called the input layer. The last layer, which provides output\n",
        "values, is called the output layer. The layers between the input layer and output\n",
        "layer are called hidden layers. Layers are numbered from left to right, starting\n",
        "with 0.\n",
        "A layer is considered dense or fully connected if each of its nodes is connected to\n",
        "each node in the next layer.\n",
        "Each node in a hidden layer is denoted hy\n",
        "x\n",
        ", where x identifies the number of the\n",
        "layer and y identifies the index of the node in the layer. For example, h1\n",
        "2 identifies\n",
        "the second node in the third layer.\n",
        "You can determine the output of each hidden node using the same methods\n",
        "dis￾cussed in the “Improving the Model” section.\n",
        "\n",
        "Deep learning:\n",
        "As you add more hidden layers to a network, it becomes capable of more sophis￾ticated\n",
        "detection and classification. When an application uses a network with\n",
        "multiple hidden layers, it’s making use of deep learning.\n",
        "Deep learning has proven effective in many applications. Two famous examples\n",
        "include Google’s AlphaGo program, which uses deep learning to beat professional\n",
        "Go players, and Google’s 2012 demonstration of an application that recognized cat\n",
        "videos on YouTube.\n",
        "Adding hidden layers to a network has two drawbacks. First, each hidden layer\n",
        "increases the amount of time needed to train the network. Second,\n",
        "each new hid￾den layer increases the chances of overfitting, which I discuss in the “Tuning the\n",
        "Neural Network” section.'''\n",
        "\n",
        "##Training with Backpropagation##\n",
        "'''\n",
        "The goal of training is to find the weights that bring y(xi\n",
        ") as close as possible to\n",
        "the observed data. Put another way, the goal is to minimize the difference between\n",
        "y(xi\n",
        ") and the observed data.\n",
        "As discussed in Chapters 5 and 6, this difference is\n",
        "called the loss, and one popular method of computing the loss is called the mean\n",
        "squared error (MSE).\n",
        "If you set y(xi\n",
        ") equal to a simple line or polynomial, you can easily compute the loss\n",
        "and pass its operation to an optimizer, such as the GradientDescentOptimizer.\n",
        "Chapter 5 covers the different optimization algorithms and their corresponding\n",
        "TensorFlow classes.\n",
        "A neural network’s model is more complicated, so the loss isn’t as easy to com￾pute.\n",
        "But in 1974, Paul Werbos was the first person to optimize the weights of a\n",
        "neural network using a method called backpropagation. Researchers have devised\n",
        "other algorithms for training neural networks since then,\n",
        "but because of its sim￾plicity and speed, backpropagation remains the most popular method.\n",
        "In essence, backpropagation extends the optimization algorithms from Chapter 5\n",
        "to apply to neural networks. The general process involves six steps:\n",
        "1. Initialize the network’s weights.\n",
        "2. For the set of inputs xi\n",
        ", compute y(xi\n",
        ").\n",
        "This computation is called forward propagation.\n",
        "3. For the set of inputs xi\n",
        ", determine the loss.\n",
        "4. For each weight, compute the partial derivative of the loss with respect\n",
        "to the weight.\n",
        "5. Using the partial derivatives computed in Step 4, update each weight in\n",
        "the network.\n",
        "6. Return to Step 2 and continue until the partial derivatives of the loss\n",
        "approach zero.\n",
        "Backpropagation extends the chain rule to partial derivatives and derivatives\n",
        "involving sums of functions. In this manner, the algorithm determines the partial\n",
        "derivative of the loss with respect to each weight in the network.\n",
        "Thankfully, you don’t need to worry about partial derivatives or the chain rule\n",
        "because TensorFlow performs backpropagation automatically. But you do need to\n",
        "create the optimizer that backpropagation will employ to update the network’s\n",
        "weights.'''\n",
        "\n",
        "##Tuning the Neural Network##\n",
        "'''\n",
        "» Input standardization: Preprocesses input data to statistically resemble\n",
        "training data\n",
        "» Weight initialization: Obtains suitable values for initial weights\n",
        "» Batch normalization: Processes data before the activation function to reduce\n",
        "the likelihood of saturation\n",
        "» Regularization: Reduces the likelihood of overfitting\n",
        "Most developers agree that neural networks require some measure of tuning, but\n",
        "few agree on the best procedure. Rather than take sides, I focus on explaining how\n",
        "you can perform operations in TensorFlow applications.\n",
        "\n",
        "Input standardization:\n",
        "A machine learning application should be able to analyze data it has never seen.\n",
        "But even if incoming data is completely new, it should have the same mean and\n",
        "standard deviation as the application’s training data. This consistency ensures\n",
        "that the application won’t be confused from one data set to the next.\n",
        "For this reason, developers frequently transform input data to set the mean equal\n",
        "to 0 and the standard deviation equal to 1. This operation is called standardization,\n",
        "and TensorFlow’s tf.nn package provides two functions that assist with stan￾dardization: moments and batch_normalization.\n",
        "moments returns a tuple containing the mean and variance of the elements in a\n",
        "tensor’s axis. Its signature is given as follows:'''\n",
        "\n",
        "moments(x, axes, shift=None, name=None, keep_dims=False)\n",
        "\n",
        "'''To set the mean and variance, assign x to the tensor to be analyzed and axes to an\n",
        "array of integers that identify the tensor’s axes. If you set keep_dims to True, the\n",
        "returned mean and variance will have the same dimensionality as the input tensor.\n",
        "batch_normalization accepts a tensor’s mean and variance and standardizes the\n",
        "tensor’s elements. Its signature is given as follows:'''\n",
        "\n",
        "batch_normalization(x, mean, variance, offset, scale, variance_\n",
        "epsilon, name=None)\n",
        "\n",
        "'''The offset parameter adds a constant to each value in the tensor, and scale\n",
        "multiplies each value by a constant. variance_epsilon identifies a value to be\n",
        "added to the denominator to ensure that TensorFlow doesn’t divide by zero.\n",
        "Applications frequently set offset to 0.0, scale to 1.0, and variance_epsilon to\n",
        "0.0001.\n",
        "For example, the following code calls moments to obtain the mean and variance of\n",
        "a tensor. Then it calls batch_normalization to obtain a new tensor with stan￾dardized data:'''\n",
        "\n",
        "#Regularization\n",
        "'''\n",
        "One of the most difficult tasks in machine learning involves finding the right\n",
        "structure for a neural network. If you add too few nodes, your network will be too\n",
        "simple to classify data accurately. This is called underfitting.\n",
        "If you add too many nodes, your network will tailor itself specifically for your\n",
        "training set and will be unsuitable for analyzing general data. This problem is\n",
        "called overfitting, and it’s a serious issue in machine learning.\n",
        "The process of updating a neural network (or other machine learning algorithm)\n",
        "to analyze general data is called regularization. Researchers have devised many\n",
        "methods for regularizing networks, and this section focuses on two:\n",
        "» Dropout: Randomly removes nodes from the network\n",
        "» L1/L2 regularization: Reduces weights by increasing the loss'''\n",
        "\n",
        "#Dropout\n",
        "'''\n",
        "The dropout process randomly removes one or more nodes from a network. For\n",
        "each node removed, dropout removes the node’s incoming and outgoing connec￾tions and their weights.\n",
        "In TensorFlow, you can configure dropout for a neural network by adding a drop￾out layer.\n",
        "Adding this layer involves calling the tf.nn.dropout function:'''\n",
        "\n",
        "dropout(x, keep_prob, noise_shape=None, seed=None, name=None)\n",
        "\n",
        "'''In this function, x is the tensor containing values from the preceding layer, and\n",
        "keep_prob is a scalar with the same type as x. The function returns a tensor with\n",
        "the same size as x.\n",
        "dropout sets each of its output values to 0 or 1/keep_prob times the correspond￾ing input value.\n",
        " More precisely, dropout sets an output value to 0 with a proba￾bility of 1-keep_prob and sets\n",
        " the output value to 1/keep_prob times the input value with a probability of keep_prob.'''\n",
        "\n",
        " #L1/L2 regularization\n",
        " '''\n",
        " L1 and L2 regularization prevent overfitting by reducing the network’s weights.\n",
        "Both methods increase the loss by a value that depends on two factors: the net￾work’s weights and a constant denoted λ.\n",
        "L1 regularization increases the loss by λ multiplied by the absolute value of the\n",
        "weight to be updated. Therefore, when the algorithm updates the weight w0\n",
        "through backpropagation, it adds a value to the loss equal to λ|w0|.\n",
        "L2 regularization increases the loss by λ/2 multiplied by the square of the weight to\n",
        "be updated. Therefore, when the algorithm updates w0, it adds λ|w0|2/2 to the loss.\n",
        "In both cases, the loss increases when the weights increase and decreases when\n",
        "the weights decrease. Therefore, the regularization process tends to reduce non￾essential weights to zero, thereby simplifying the model and (hopefully) avoiding\n",
        "overfitting.'''\n",
        "To perform L1/L2 regularization in TensorFlow, you can call tf.contrib.layers.\n",
        "l1_regularizer or tf.contrib.layers.l2_regularizer:\n",
        "» l1_regularizer(lambda, scope=None): Returns a function that performs\n",
        "L1 regularization\n",
        "» l2_regularizer(lambda, scope=None): Returns a function that performs\n",
        "L2 regularization\n",
        "'''These functions return special functions called regularizers. After you’ve obtained\n",
        "a regularizer, you can regularize a set of weights by calling tf.contrib.layers.\n",
        "apply_regularization:\n",
        "apply_regularization(regularizer, weights_list=None)\n",
        "Many TensorFlow functions accept regularizers as arguments. One important\n",
        "function is tf.contrib.layers.fully_connected, which I discuss in the\n",
        "“Improving the Deep Learning Process” section.'''\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Implementing Deep Learning*"
      ],
      "metadata": {
        "id": "ZRp9UINRLgJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the TensorFlow library\n",
        "import tensorflow as tf\n",
        "\n",
        "# Read MNIST data\n",
        "dataset = learn.datasets.mnist.read_data_sets('MNIST-data', one_hot=True)\n",
        "\n",
        "# Create placeholders for MNIST images and labels\n",
        "img_holder = tf.placeholder(tf.float32, [None, 784])\n",
        "lbl_holder = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# Set the number of hidden nodes and output nodes\n",
        "hid_nodes = 200\n",
        "out_nodes = 10\n",
        "\n",
        "# Define weights and biases for each layer\n",
        "w0 = tf.Variable(tf.random_normal([784, hid_nodes]))\n",
        "w1 = tf.Variable(tf.random_normal([hid_nodes, hid_nodes]))\n",
        "w2 = tf.Variable(tf.random_normal([hid_nodes, hid_nodes]))\n",
        "w3 = tf.Variable(tf.random_normal([hid_nodes, out_nodes]))\n",
        "\n",
        "b0 = tf.Variable(tf.random_normal([hid_nodes]))\n",
        "b1 = tf.Variable(tf.random_normal([hid_nodes]))\n",
        "b2 = tf.Variable(tf.random_normal([hid_nodes]))\n",
        "b3 = tf.Variable(tf.random_normal([out_nodes]))\n",
        "\n",
        "# Create layers using the defined weights and biases\n",
        "layer_1 = tf.add(tf.matmul(img_holder, w0), b0)\n",
        "layer_1 = tf.nn.relu(layer_1)\n",
        "layer_2 = tf.add(tf.matmul(layer_1, w1), b1)\n",
        "layer_2 = tf.nn.relu(layer_2)\n",
        "layer_3 = tf.add(tf.matmul(layer_2, w2), b2)\n",
        "layer_3 = tf.nn.relu(layer_3)\n",
        "out_layer = tf.matmul(layer_3, w3) + b3\n",
        "\n",
        "# Compute the loss using softmax cross-entropy\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=lbl_holder))\n",
        "\n",
        "# Create an optimizer (Adam optimizer) to minimize the loss\n",
        "learning_rate = 0.01\n",
        "num_epochs = 15\n",
        "batch_size = 100\n",
        "num_batches = int(dataset.train.num_examples / batch_size)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "\n",
        "# Initialize variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch a TensorFlow session\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "\n",
        "    # Loop over epochs\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Loop over batches\n",
        "        for batch in range(num_batches):\n",
        "            img_batch, lbl_batch = dataset.train.next_batch(batch_size)\n",
        "            sess.run(optimizer, feed_dict={img_holder: img_batch, lbl_holder: lbl_batch})\n",
        "\n",
        "    # Determine the success rate\n",
        "    prediction = tf.equal(tf.argmax(out_layer, 1), tf.argmax(lbl_holder, 1))\n",
        "    success = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
        "    print('Success rate: ', sess.run(success, feed_dict={img_holder: dataset.test.images, lbl_holder: dataset.test.labels}))\n",
        "\n",
        "\n",
        "'''In this code:\n",
        "\n",
        "The TensorFlow library is imported.\n",
        "The MNIST data is read and stored in the dataset variable.\n",
        "Placeholders are created for the MNIST images (img_holder) and labels (lbl_holder).\n",
        "The number of hidden nodes (hid_nodes) and output nodes (out_nodes) is specified.\n",
        "Weights and biases are defined for each layer.\n",
        "Layers are created using the defined weights and biases, with ReLU activation applied to each layer except the output layer.\n",
        "The loss is computed using softmax cross-entropy.\n",
        "An Adam optimizer is created with a specified learning rate.\n",
        "Variables are initialized.\n",
        "A TensorFlow session is launched.\n",
        "The model is trained for a specified number of epochs and batches.\n",
        "The success rate is determined by comparing the predicted labels with the true labels, and the result is printed.\n",
        "'''"
      ],
      "metadata": {
        "id": "g5nx7YZELsap"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}